{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2e5dc9-20fe-4b57-be4e-6487300fbca3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Graph Library (DGL) test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40dae6-610d-4d02-b06c-b53f4af86cf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bf53a7e-210a-48d6-9420-85ce6782d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"dgl.ipynb\" #CHANGEEEEEEEEEEEEEEE\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from dgl.data import DGLDataset\n",
    "from dgl import save_graphs, load_graphs\n",
    "from dgl.data.utils import makedirs, save_info, load_info, split_dataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError\n",
    "from torchmetrics.regression import SymmetricMeanAbsolutePercentageError\n",
    "from dgl import RowFeatNormalizer\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datanetAPI import DatanetAPI\n",
    "import requests\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "from numpy import log as ln\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d0d904b-d2b1-4540-bc9f-c782f34744ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/escenari-slicing/samples-UPC/V2-full'\n",
    "save_dataset_path = './V2-samples-fulldataset'\n",
    "save_model_path = './V2-model-first-long-train-fulldataset-multipleprediction-leaky'\n",
    "dataset_name=\"Slicing-V2-fulldataset-865\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea14aae-4e81-4286-8812-78c0bf0f2854",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weights and Biases + Telegram config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a1d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start a W&B run\n",
    "# wandb.init(project=\"DGL-Slicing\", entity=\"mfarreras\",     \n",
    "#            # track hyperparameters and run metadata\n",
    "#             config={\n",
    "#                 \"learning_rate\": 0.001,\n",
    "#                 \"model\": \"DESCONEGUT\",\n",
    "#                 \"dataset\": \"TRAIN ANTIC\",\n",
    "#                 \"epochs\": 10,\n",
    "#             })\n",
    "\n",
    "#wandb.run.name = \"First model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5beec1d6-1032-4b37-8804-9440a25272ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_config.learning_rate = float(config['HYPERPARAMETERS']['learning_rate'])\n",
    "# wandb_config.logs = config['DIRECTORIES']['logs']\n",
    "# wandb_config.val = config['DIRECTORIES']['val']\n",
    "# wandb_config.train = config['DIRECTORIES']['train']\n",
    "# wandb_config.link_state_dim = config['HYPERPARAMETERS']['link_state_dim']\n",
    "# wandb_config.path_state_dim = config['HYPERPARAMETERS']['path_state_dim']\n",
    "# wandb_config.t = config['HYPERPARAMETERS']['t']\n",
    "# wandb_config.readout_units = config['HYPERPARAMETERS']['readout_units']\n",
    "# wandb_config.steps_per_epoch = config['RUN_CONFIG']['steps_per_epoch']\n",
    "# wandb_config.validation_steps = config['RUN_CONFIG']['validation_steps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef1f91-4940-4ecd-b2d5-f0c117ff4db6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cbfb77f7-00e0-4d00-be4f-9a515603b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicingDataset(DGLDataset):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_dir : str\n",
    "        Specifying the directory that will store the\n",
    "        downloaded data or the directory that\n",
    "        already stores the input data.\n",
    "    save_dir : str\n",
    "        Directory to save the processed dataset.\n",
    "        Default: the value of `raw_dir`\n",
    "    force_reload : bool\n",
    "        Whether to reload the dataset. Default: False\n",
    "    verbose : bool\n",
    "        Whether to print out progress information\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, \n",
    "                 name=\"default_name\", \n",
    "                 raw_dir=None, \n",
    "                 save_dir=None, \n",
    "                 force_reload=False):\n",
    "        self.tool = DatanetAPI(raw_dir, shuffle=False)\n",
    "        super(SlicingDataset, self).__init__(\n",
    "            name=name, \n",
    "            raw_dir=raw_dir, \n",
    "            save_dir=save_dir, \n",
    "            force_reload=force_reload)\n",
    "        \n",
    "\n",
    "    def process(self):\n",
    "        it = iter(self.tool)\n",
    "        self.graphs = []\n",
    "        for s in tqdm(it, total=len(self.tool.get_available_files()*7)):\n",
    "            #obtain edge and graph data\n",
    "            #how to save path data?\n",
    "            G = nx.DiGraph(s.get_topology_object().copy())\n",
    "            R = s.get_routing_matrix()\n",
    "            T = s.get_traffic_matrix() \n",
    "            D = s.get_performance_matrix()\n",
    "            P = s.get_port_stats()\n",
    "            S = s.get_slices()\n",
    "            D_G = nx.DiGraph()\n",
    "\n",
    "            #AvgBw_sum calculation\n",
    "            AvgBw_sum = np.zeros((len(G.nodes),len(G.nodes)))\n",
    "            for src in range(G.number_of_nodes()):\n",
    "                for dst in range(G.number_of_nodes()):\n",
    "                    route = R[src,dst]\n",
    "                    if route:\n",
    "                        for index, node in enumerate(route):\n",
    "                                next_node_index = index + 1\n",
    "                                if next_node_index < len(route):\n",
    "                                    for f_id in range(len(T[src, dst]['Flows'])):\n",
    "                                        if T[src, dst]['Flows'][f_id]['AvgBw'] != 0 and T[src, dst]['Flows'][f_id]['PktsGen'] != 0:\n",
    "                                            AvgBw_sum[node][route[next_node_index]] += T[src, dst]['Flows'][f_id]['AvgBw']\n",
    "\n",
    "            slice_types = np.zeros((len(G.nodes),len(G.nodes)))\n",
    "            reservations_embb = np.zeros((len(G.nodes),len(G.nodes)))\n",
    "            reservations_urllc = np.zeros((len(G.nodes),len(G.nodes)))\n",
    "            reservations_mmtc = np.zeros((len(G.nodes),len(G.nodes)))\n",
    "            for slice in S:\n",
    "                for flow in slice[\"flows\"]:\n",
    "                    route = literal_eval(flow[\"path\"])\n",
    "                    if slice[\"type\"] == \"eMBB\":\n",
    "                        sliceType = 0\n",
    "                    elif slice[\"type\"] == \"URLLC\":\n",
    "                        sliceType = 1\n",
    "                    else :\n",
    "                        sliceType = 2\n",
    "                    slice_types[route[0]][route[len(route)-1]] = sliceType\n",
    "                    for index, node in enumerate(route):\n",
    "                        next_node_index = index + 1\n",
    "                        if next_node_index < len(route):\n",
    "                            if (slice[\"type\"] == \"eMBB\"):\n",
    "                                v_max = -float(flow[\"bandwidth\"])/ln(0.50)\n",
    "                                v_min = -float(flow[\"bandwidth\"])/ln(0.05)\n",
    "                                reservations_embb[node][route[next_node_index]] += 0.5*v_min+slice[\"delta\"]*(1.5*v_max-0.5*v_min)\n",
    "                            elif (slice[\"type\"] == \"URLLC\"):\n",
    "                                reservations_urllc[node][route[next_node_index]] += flow[\"bandwidth\"]\n",
    "                            else:\n",
    "                                reservations_mmtc[node][route[next_node_index]] += flow[\"bandwidth\"]\n",
    "            \n",
    "            for src in range(G.number_of_nodes()):\n",
    "                for dst in range(G.number_of_nodes()):\n",
    "                    if src != dst:\n",
    "        \n",
    "                        if G.has_edge(src, dst):\n",
    "                            D_G.add_node('l_{}_{}'.format(src, dst),\n",
    "                                        capacity=torch.tensor(G.edges[src, dst]['bandwidth']),\n",
    "                                        utilization=torch.tensor(P[src][dst][\"utilization\"]),\n",
    "                                        losses=torch.tensor(P[src][dst][\"losses\"]),\n",
    "                                        avgPacketSize=torch.tensor(P[src][dst][\"avgPacketSize\"]), \n",
    "                                        offeredTrafficIntensity=torch.tensor(AvgBw_sum[src][dst]/s.get_srcdst_link_bandwidth(src,dst)),#offeredTrafficIntensity\n",
    "                                        embbReservations=torch.tensor(reservations_embb[src][dst]), #convertir a tant per 1 potser no ens va bé i millor a cues\n",
    "                                        urllcReservations=torch.tensor(reservations_urllc[src][dst]),\n",
    "                                        mmtcReservations=torch.tensor(reservations_mmtc[src][dst]),\n",
    "                                        nodeType=torch.tensor(0)\n",
    "                                        )\n",
    "                            # for queue in queues:\n",
    "                            #     new node with queue attributes\n",
    "                            #     add edges from queues to links and from queues to flows\n",
    "                            #     D_G.add_edge('p_{}_{}_{}'.format(src, dst, f_id), 'l_{}_{}'.format(h_1, h_2))\n",
    "                            #     D_G.add_edge('l_{}_{}'.format(h_1, h_2), 'p_{}_{}_{}'.format(src, dst, f_id))\n",
    "            for src in range(G.number_of_nodes()):\n",
    "                for dst in range(G.number_of_nodes()):\n",
    "                    if src != dst:\n",
    "                        for f_id in range(len(T[src, dst]['Flows'])):\n",
    "                            if T[src, dst]['Flows'][f_id]['AvgBw'] != 0 and T[src, dst]['Flows'][f_id]['PktsGen'] != 0:\n",
    "                                D_G.add_node('p_{}_{}_{}'.format(src, dst, f_id),\n",
    "                                            traffic=torch.tensor(T[src, dst]['Flows'][f_id]['AvgBw']),\n",
    "                                            packets=torch.tensor(T[src, dst]['Flows'][f_id]['PktsGen']),\n",
    "                                            delay=torch.tensor(D[src, dst]['Flows'][f_id]['AvgDelay']),\n",
    "                                            jitter=torch.tensor(float(D[src, dst]['Flows'][f_id]['Jitter'])),\n",
    "                                            pathLength=torch.tensor(len(R[src,dst])),\n",
    "                                            drops=torch.tensor(D[src, dst]['AggInfo']['PktsDrop']/T[src, dst]['Flows'][f_id]['PktsGen']),\n",
    "                                            delta=torch.tensor(slice[\"delta\"]),#afegir reserva\n",
    "                                            sliceType=torch.tensor(slice_types[src][dst]),\n",
    "                                            nodeType=torch.tensor(1)\n",
    "                                            )\n",
    "        \n",
    "                                for h_1, h_2 in [R[src, dst][i:i + 2] for i in range(0, len(R[src, dst]) - 1)]:\n",
    "                                    D_G.add_edge('p_{}_{}_{}'.format(src, dst, f_id), 'l_{}_{}'.format(h_1, h_2), edgeType=torch.tensor(0))\n",
    "                                    D_G.add_edge('l_{}_{}'.format(h_1, h_2), 'p_{}_{}_{}'.format(src, dst, f_id), edgeType=torch.tensor(1))\n",
    "\n",
    "                        #afegir queues i slices vinculats a flows\n",
    "                        #occupancy=P[src][dst]['qosQueuesStats'][0]['avgPortOccupancy']/G.nodes[src]['queueSizes'],\n",
    "                        #drops=float(D[src, dst]['AggInfo']['PktsDrop'])/float(T[src, dst]['Flows'][0]['PktsGen']))\n",
    "        \n",
    "            D_G.remove_nodes_from([node for node, out_degree in D_G.out_degree() if out_degree == 0])\n",
    "            \n",
    "            g = dgl.from_networkx(D_G, node_attrs=[\"nodeType\"])\n",
    "\n",
    "            nodeTypes = nx.get_node_attributes(D_G, 'nodeType')\n",
    "            nodes = D_G.nodes\n",
    "            g.ndata[dgl.NTYPE] = torch.stack([nodeTypes[e] for e in nodes])\n",
    "            g.ndata[dgl.NID] = torch.tensor([*range(0,len(D_G.nodes))])\n",
    "\n",
    "            edgeTypes = nx.get_edge_attributes(D_G, 'edgeType')\n",
    "            edges = D_G.edges\n",
    "            g.edata[dgl.ETYPE] = torch.stack([edgeTypes[e] for e in edges])\n",
    "            g.edata[dgl.EID] = torch.tensor([*range(0,len(D_G.edges))])\n",
    "            g = dgl.to_heterogeneous(g, ['link','path'], ['traverses','composes'])\n",
    "\n",
    "            capacities, utilizations, losses, avgPacketSizes, offeredTrafficIntensities, embbReservations, urllcReservations, mmtcReservations, nodeTypesLink = ([] for i in range(9))\n",
    "            \n",
    "            traffics, packets, delays, jitters, pathLengths, drops, deltas, nodeTypesPath, sliceTypesPath = ([] for i in range(9))\n",
    "            \n",
    "            for node in D_G.nodes(data=True):\n",
    "                if node[1][\"nodeType\"] == 0: #LINK               \n",
    "                    capacities.append(node[1][\"capacity\"])\n",
    "                    utilizations.append(node[1][\"utilization\"])\n",
    "                    losses.append(node[1][\"losses\"])\n",
    "                    avgPacketSizes.append(node[1][\"avgPacketSize\"])\n",
    "                    offeredTrafficIntensities.append(node[1][\"offeredTrafficIntensity\"])\n",
    "                    embbReservations.append(node[1][\"embbReservations\"])\n",
    "                    urllcReservations.append(node[1][\"urllcReservations\"])\n",
    "                    mmtcReservations.append(node[1][\"mmtcReservations\"])\n",
    "                    nodeTypesLink.append(node[1][\"nodeType\"])\n",
    "            \n",
    "                else :#if node[1][\"type\"] == 1: #FLOW/PATH\n",
    "                    traffics.append(node[1][\"traffic\"])\n",
    "                    packets.append(node[1][\"packets\"])\n",
    "                    delays.append(node[1][\"delay\"])\n",
    "                    jitters.append(node[1][\"jitter\"])\n",
    "                    pathLengths.append(node[1][\"pathLength\"])\n",
    "                    drops.append(node[1][\"drops\"])\n",
    "                    deltas.append(node[1][\"delta\"])\n",
    "                    nodeTypesPath.append(node[1][\"nodeType\"])\n",
    "                    sliceTypesPath.append(node[1][\"sliceType\"])\n",
    "\n",
    "            g.nodes[\"link\"].data[\"capacity\"] = torch.stack(capacities)\n",
    "            g.nodes[\"link\"].data[\"utilization\"] = torch.stack(utilizations)\n",
    "            g.nodes[\"link\"].data[\"losses\"] = torch.stack(losses)\n",
    "            g.nodes[\"link\"].data[\"avgPacketSize\"] = torch.stack(avgPacketSizes)\n",
    "            g.nodes[\"link\"].data[\"offeredTrafficIntensity\"] = torch.stack(offeredTrafficIntensities)\n",
    "            g.nodes[\"link\"].data[\"embbReservations\"] = torch.stack(embbReservations)\n",
    "            g.nodes[\"link\"].data[\"urllcReservations\"] = torch.stack(urllcReservations)\n",
    "            g.nodes[\"link\"].data[\"mmtcReservations\"] = torch.stack(mmtcReservations)\n",
    "            g.nodes[\"link\"].data[\"nodeType\"] = torch.stack(nodeTypesLink)\n",
    "            \n",
    "            g.nodes[\"path\"].data[\"traffic\"] = torch.stack(traffics)\n",
    "            g.nodes[\"path\"].data[\"packets\"] = torch.stack(packets)\n",
    "            g.nodes[\"path\"].data[\"delay\"] = torch.stack(delays)\n",
    "            g.nodes[\"path\"].data[\"jitter\"] = torch.stack(jitters)\n",
    "            g.nodes[\"path\"].data[\"pathLength\"] = torch.stack(pathLengths)\n",
    "            g.nodes[\"path\"].data[\"drops\"] = torch.stack(drops)\n",
    "            g.nodes[\"path\"].data[\"delta\"] = torch.stack(deltas)\n",
    "            g.nodes[\"path\"].data[\"nodeType\"] = torch.stack(nodeTypesPath)\n",
    "            g.nodes[\"path\"].data[\"sliceType\"] = torch.stack(sliceTypesPath)\n",
    "\n",
    "            self.graphs.append(g)\n",
    "            if len(self.graphs) > 865:\n",
    "                break\n",
    "        self.save()\n",
    "        \n",
    "    def getGraphs(self):\n",
    "        return self.graphs\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get graph and label by index\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (dgl.DGLGraph)\n",
    "        \"\"\"\n",
    "        return self.graphs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of graphs in the dataset, *7 because the samples are grouped in groups of 7\"\"\"\n",
    "        return len(self.tool.get_available_files())*7-1\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # batch is a list of tuple (graph, label)\n",
    "        graphs = [e[0] for e in batch]\n",
    "        g = dgl.batch(graphs)\n",
    "        labels = [e[1] for e in batch]\n",
    "        labels = torch.stack(labels, 0)\n",
    "        return g\n",
    "\n",
    "    def save(self):\n",
    "        # save graphs and labels\n",
    "        graph_path = os.path.join(self.save_path, 'dgl_graph.bin')\n",
    "        save_graphs(graph_path, self.graphs)\n",
    "        \n",
    "    def load(self):\n",
    "        # load processed data from directory `self.save_path`\n",
    "        graph_path = os.path.join(self.save_path, 'dgl_graph.bin')\n",
    "        self.graphs = load_graphs(graph_path)\n",
    "\n",
    "\n",
    "    def has_cache(self):\n",
    "        # check whether there is processed data in `self.save_path`\n",
    "        graph_path = os.path.join(self.save_path, 'dgl_graph.bin')\n",
    "        return os.path.exists(graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45330206-2163-469d-8cfb-62b9810ee336",
   "metadata": {},
   "source": [
    "1. Obtenir graph base dirigit de cada sample  \n",
    "2. Afegir atributs desitjats al graf\n",
    "3. Carregar-lo a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19508292-17b7-4566-90b0-d46bfcd3574f",
   "metadata": {},
   "source": [
    "## Dataloader initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "dataset = SlicingDataset(\n",
    "    name=dataset_name,\n",
    "    raw_dir=dataset_path, \n",
    "    save_dir=save_dataset_path, \n",
    "    force_reload=False)\n",
    "\n",
    "dataset.load()\n",
    "ds_train, ds_validation, ds_test = split_dataset(dataset,[0.8, 0.1, 0.1]) #JUST FOR TEST\n",
    "#ds_train, ds_validation, ds_test = split_dataset(dataset,[0, 0, 1]) #JUST FOR TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4434ab4f-e037-4841-9af4-d47c56662107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set dataloader\n",
    "#CANVIAR BATCH SIZE\n",
    "#dataloader_train = GraphDataLoader(ds_train, batch_size=1, shuffle=False, collate_fn=dataset.collate_fn)\n",
    "#dataloader_validation = GraphDataLoader(ds_validation, batch_size=1, shuffle=False, collate_fn=dataset.collate_fn)\n",
    "#dataloader_test = GraphDataLoader(ds_test, batch_size=1, shuffle=False, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad39e3-1a44-4204-a171-d7bd35052733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4687eca-e8fa-41a6-889a-0e5d616d43e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "8637\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_validation))\n",
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87acca5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd2eab5f-a770-4ad0-a139-73e581976259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Heterograph Conv model\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats_link, in_feats_path, hid_feats, out_feats_link, out_feats_path, rel_names):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            'traverses': dglnn.GraphConv(in_feats_path, hid_feats),\n",
    "            'composes': dglnn.GraphConv(in_feats_link, hid_feats)},\n",
    "            aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            'traverses': dglnn.GraphConv(hid_feats, out_feats_path),\n",
    "            'composes': dglnn.GraphConv(hid_feats, out_feats_link)},\n",
    "            aggregate='sum')\n",
    "  \n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.leaky_relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c362b776-d487-4f5c-addc-e42b0242a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinFeatures(fs):\n",
    "    features = []\n",
    "    for feature in fs:\n",
    "        if feature == \"_ID\" or feature == \"nodeType\" or feature == \"utilization\" or feature == \"sliceType\":\n",
    "            continue\n",
    "        else:\n",
    "            features.append(fs[feature])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "153d402e-9d6a-4fc1-8d78-94263599503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGCN(\n",
       "  (conv1): HeteroGraphConv(\n",
       "    (mods): ModuleDict(\n",
       "      (traverses): GraphConv(in=7, out=20, normalization=both, activation=None)\n",
       "      (composes): GraphConv(in=7, out=20, normalization=both, activation=None)\n",
       "    )\n",
       "  )\n",
       "  (conv2): HeteroGraphConv(\n",
       "    (mods): ModuleDict(\n",
       "      (traverses): GraphConv(in=20, out=3, normalization=both, activation=None)\n",
       "      (composes): GraphConv(in=20, out=3, normalization=both, activation=None)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RGCN(7,7, 20, 3,3, ['traverses','composes'])\n",
    "model.to(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-christianity",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "630c5f33-77e7-4952-9649-66bcf32d85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "node_feats = [\"avgPacketSize\",\"embbReservations\",\"urllcReservations\",\"mmtcReservations\",\"traffic\",\"packets\",\"delay\",\"jitter\",\"drops\",\"delta\"]\n",
    "\n",
    "transform = RowFeatNormalizer(subtract_min=True, node_feat_names=node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6465643-7b36-47bb-b7a8-24db32a470e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss_object = nn.MSELoss()\n",
    "\n",
    "metric = MeanAbsolutePercentageError()\n",
    "loss_values = []\n",
    "val_values_delay = []\n",
    "val_values_drops = []\n",
    "val_values_jitter = []\n",
    "batch_size = 100\n",
    "i = 0\n",
    "for epoch in tqdm(range(2000)): #wandb.config[\"epochs\"] #5000+2000\n",
    "    for g in random.sample(ds_train[0], batch_size):\n",
    "        g = transform(g)\n",
    "        \n",
    "        link_feats = torch.transpose(torch.stack(joinFeatures(g.nodes['link'].data)),0,1).contiguous()\n",
    "        path_feats = torch.transpose(torch.stack(joinFeatures(g.nodes['path'].data)),0,1).contiguous()\n",
    "        labels = torch.stack((g.nodes['path'].data['delay'], g.nodes['path'].data['jitter'], g.nodes['path'].data['drops']))\n",
    "\n",
    "        node_features = {'link': link_feats, 'path': path_feats}    \n",
    "        model.train()\n",
    "        # forward propagation by using all nodes and extracting the path embeddings\n",
    "        logits = model(g, node_features)['path']\n",
    "        logits = torch.transpose(logits,0,1)\n",
    "        # compute loss\n",
    "        loss = loss_object(logits, labels)\n",
    "        loss_values.append(loss.item())\n",
    "        # backward propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        i+=1\n",
    "    #break\n",
    "    # validate model\n",
    "    with torch.no_grad():\n",
    "        for g in random.sample(ds_validation.dataset[0], batch_size):\n",
    "            g = transform(g)\n",
    "            link_feats = torch.transpose(torch.stack(joinFeatures(g.nodes['link'].data)),0,1)\n",
    "            path_feats = torch.transpose(torch.stack(joinFeatures(g.nodes['path'].data)),0,1)\n",
    "            labels = torch.stack((g.nodes['path'].data['delay'], g.nodes['path'].data['jitter'], g.nodes['path'].data['drops']))\n",
    "            node_features = {'link': link_feats, 'path': path_feats}\n",
    "            #out_data = torch.index_select(model(g,node_features)['path'],1,torch.tensor([2])).squeeze(1)\n",
    "            logits = model(g, node_features)['path']\n",
    "            logits = torch.transpose(logits,0,1)\n",
    "            mape_delay = metric(logits[0], labels[0])\n",
    "            val_values_delay.append(mape_delay.item())\n",
    "            mape_jitter = metric(logits[1], labels[1])\n",
    "            val_values_jitter.append(mape_jitter.item())\n",
    "            mape_drops = metric(logits[2], labels[2])\n",
    "            val_values_drops.append(mape_drops.item())\n",
    "            \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), save_model_path)\n",
    "    if epoch%100 == 0:\n",
    "        print(mape_delay)\n",
    "        print(mape_jitter)\n",
    "        print(mape_drops)\n",
    "    #wandb.log({\"loss\": loss, \"MAPE_delay\": mape, \"MAPE_jitter\": mape_jitter, \"MAPE_drops\": mape_drops})\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-colon",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "altered-recommendation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGCN(\n",
       "  (conv1): HeteroGraphConv(\n",
       "    (mods): ModuleDict(\n",
       "      (traverses): GraphConv(in=7, out=20, normalization=both, activation=None)\n",
       "      (composes): GraphConv(in=7, out=20, normalization=both, activation=None)\n",
       "    )\n",
       "  )\n",
       "  (conv2): HeteroGraphConv(\n",
       "    (mods): ModuleDict(\n",
       "      (traverses): GraphConv(in=20, out=3, normalization=both, activation=None)\n",
       "      (composes): GraphConv(in=20, out=3, normalization=both, activation=None)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-blanket",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21d7e155-c95c-4544-b9ef-f91f0de616ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanAbsolutePercentageError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "anonymous-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4466090202331543\n",
      "MAPE delay: 1.6823573528204716\n",
      "MAPE jitter: 0.794666419700065\n",
      "MAPE drops: 1.977382451851295\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "test_values_delay = []\n",
    "test_values_drops = []\n",
    "test_values_jitter = []\n",
    "\n",
    "def find_indices(lst, element):\n",
    "    indices = [index for index, value in enumerate(lst) if value == element]\n",
    "    return indices\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for g in ds_test.dataset[0]:\n",
    "        g = transform(g)\n",
    "        link_feats = torch.transpose(torch.stack(joinFeatures(g.nodes['link'].data)),0,1)\n",
    "        path_feats = torch.transpose(torch.stack(joinFeatures(g.nodes['path'].data)),0,1)\n",
    "        labels = torch.stack((g.nodes['path'].data['delay'], g.nodes['path'].data['jitter'], g.nodes['path'].data['drops']))\n",
    "        node_features = {'link': link_feats, 'path': path_feats}\n",
    "        \n",
    "        logits = model(g, node_features)['path']\n",
    "        logits = torch.transpose(logits,0,1)\n",
    "        logits[0] = torch.nn.functional.relu(logits[0].clone(), inplace=True)\n",
    "        logits[1] = torch.nn.functional.relu(logits[1].clone(), inplace=True)\n",
    "        logits[2] = torch.nn.functional.relu(logits[2].clone(), inplace=True)\n",
    "        sliceTypes = g.nodes['path'].data[\"sliceType\"].tolist()\n",
    "        #print(sliceTypes)\n",
    "        indices = find_indices(sliceTypes, 1)\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            #print(indices)\n",
    "            logits_0 = torch.index_select(logits[0], 0, torch.tensor(indices))\n",
    "            labels_0 = torch.index_select(labels[0], 0, torch.tensor(indices))\n",
    "            logits_1 = torch.index_select(logits[1], 0, torch.tensor(indices))\n",
    "            labels_1 = torch.index_select(labels[1], 0, torch.tensor(indices))\n",
    "            logits_2 = torch.index_select(logits[2], 0, torch.tensor(indices))\n",
    "            labels_2 = torch.index_select(labels[2], 0, torch.tensor(indices))\n",
    "            \n",
    "            mape_delay = metric(logits_0, labels_0)\n",
    "            test_values_delay.append(mape_delay.item())\n",
    "            mape_jitter = metric(logits_1, labels_1)\n",
    "            test_values_jitter.append(mape_jitter.item())\n",
    "            mape_drops = metric(logits_2, labels_2)\n",
    "            test_values_drops.append(mape_drops.item())\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "        \n",
    "print(\"MAPE delay: \"+str(sum(test_values_delay)/len(test_values_delay)))\n",
    "print(\"MAPE jitter: \"+str(sum(test_values_jitter)/len(test_values_jitter)))\n",
    "print(\"MAPE drops: \"+str(sum(test_values_drops)/len(test_values_drops)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“pytorch”",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
